# sympy examples

"""NOTES
In short, we can think of the cross-entropy classification objective in two ways: (i) as maximizing the likelihood of the observed data; and (ii) as minimizing our surprisal (and thus the number of bits) required to communicate the labels.
"""



"""
3.4.9. Exercises
1. We can explore the connection between exponential families and the softmax in some more depth.

    1.1 Compute the second derivative of the cross-entropy loss  ğ‘™(ğ²,ğ²Ì‚ )  for the softmax.

    1.2 Compute the variance of the distribution given by  softmax(ğ¨)  and show that it matches the second derivative computed above.

2. Assume that we have three classes which occur with equal probability, i.e., the probability vector is  (13,13,13) .

    What is the problem if we try to design a binary code for it?

    Can you design a better code? Hint: what happens if we try to encode two independent observations? What if we encode  ğ‘›  observations jointly?

3. Softmax is a misnomer for the mapping introduced above (but everyone in deep learning uses it). The real softmax is defined as  RealSoftMax(ğ‘,ğ‘)=log(exp(ğ‘)+exp(ğ‘)) .

    Prove that  RealSoftMax(ğ‘,ğ‘)>max(ğ‘,ğ‘) .

    Prove that this holds for  ğœ†âˆ’1RealSoftMax(ğœ†ğ‘,ğœ†ğ‘) , provided that  ğœ†>0 .

    Show that for  ğœ†â†’ âˆ  we have  ğœ†âˆ’1RealSoftMax(ğœ†ğ‘,ğœ†ğ‘)â†’ max(ğ‘,ğ‘) .

    What does the soft-min look like?

    Extend this to more than two numbers.
"""

"""
1. We can explore the connection between exponential families and the softmax in some more depth.

    Compute the second derivative of the cross-entropy loss  ğ‘™(ğ²,ğ²Ì‚)  for the softmax.

note:
    j is contained in k
    k are the classification classes

From the text we have for the loss:
ğ‘™(ğ²,ğ²Ì‚ ) = log(âˆ‘_ğ‘˜ exp(o_k) ) - âˆ‘_j(y_j o_j)

and its first derivative:

âˆ‚ğ‘œ_ğ‘— ğ‘™(ğ²,ğ²Ì‚ ) =

    = exp(ğ‘œ_ğ‘—)/âˆ‘_ğ‘˜(exp(ğ‘œ_ğ‘˜)) âˆ’ ğ‘¦_ğ‘—

    = softmax(ğ¨)_ğ‘— âˆ’ ğ‘¦_ğ‘—.


0. Solve for first derivative âˆ‚ğ‘œ_ğ‘— ğ‘™(ğ²,ğ²Ì‚ ):

    âˆ‚ğ‘œ_ğ‘— ğ‘™(ğ²,ğ²Ì‚ ) = log(âˆ‘_ğ‘˜(exp(o_k))) - âˆ‘_j(y_j o_j)

    do_j log(âˆ‘_ğ‘˜(e^o_k)) - do_j( âˆ‘_j(y_j o_j) )

    do_j log(âˆ‘_ğ‘˜(exp(o_k))) - âˆ‘_j(do_j( y_j o_j ))

    do_j log(âˆ‘_ğ‘˜(exp(o_k))) - âˆ‘_j(y_j)

    note:
        do_j âˆ‘_ğ‘˜ exp(o_k) == exp(o_j)

    âˆ‘_ğ‘˜(exp(o_k))^-1 * exp(o_j) - âˆ‘_j(y_j)

    note:
        softmax(o)_j := âˆ‘_ğ‘˜(exp(o_k))^-1 * exp(o_k)
        âˆ‘_j(y_j) =?= y_j

    âˆ‚ğ‘œ_ğ‘— ğ‘™(ğ²,ğ²Ì‚ ) = softmax(o)_j - y_j


1. Solve for the second derivative âˆ‚2ğ‘œ_ğ‘—
    (or first derivative of the derivative from above):

    âˆ‚ğ‘œ_ğ‘—( âˆ‘_ğ‘˜(exp(o_k))^-1 * exp(o_j) - y_j )

    âˆ‚ğ‘œ_ğ‘— âˆ‘_ğ‘˜(exp(o_k))^-1 * exp(o_j) - âˆ‚ğ‘œ_ğ‘— y_j

    âˆ‚ğ‘œ_ğ‘— âˆ‘_ğ‘˜(exp(o_k))^-1 * exp(o_j) - 0

    âˆ‚ğ‘œ_ğ‘— âˆ‘_ğ‘˜(exp(o_k))^-1 * exp(o_j) - 0

    âˆ‚ğ‘œ_ğ‘—( e^o_j/âˆ‘_ğ‘˜ e^o_k )

    note:
        quotient rule
        dx (N/D) = (DN' - ND')/D'^2, where N and D are functions of x

        and from above
        do_j âˆ‘_ğ‘˜ exp(o_k) == exp(o_j)

    ( âˆ‘_ğ‘˜(e^o_k * e^o_j) - e^o_j * e^o_j ) / e^(o_j*2)

    e^o_j ( âˆ‘_ğ‘˜ e^o_k - e^o_j ) / e^(o_j*2)

    ( âˆ‘_ğ‘˜ e^o_k - e^o_j ) / e^(o_j)

    âˆ‚2ğ‘œ_ğ‘— = ( âˆ‘_ğ‘˜ e^o_k - e^o_j ) / e^(o_j)


2. Compute the variance of the distribution given by softmax(ğ¨) and show that it matches the second derivative computed above.


var(softmax(o)) =

Recall the expectation is computed as:

    ğ¸[ğ‘‹] = âˆ‘_ğ‘¥(ğ‘¥ ğ‘ƒ(ğ‘‹=ğ‘¥))

Let's start with the expectation of softmax(o):

softmax(o) := exp(o_j)/âˆ‘_ğ‘˜(exp(o_k))

note:
    softmax(o) is a distribution:

E[softmax(o)] =  âˆ‘_o o*exp(o)/âˆ‘_ğ‘˜(exp(o))


And the variance is:

    Var[ğ‘‹] = ğ¸[(ğ‘‹ âˆ’ ğ¸[ğ‘‹])^2] = ğ¸[ğ‘‹^2]âˆ’ğ¸[ğ‘‹]^2

Var[softmax(o)] =  ğ¸[softmax(o^2)]âˆ’ğ¸[softmax(o)]^2

âˆ‘_o o^2*exp(o^2)/âˆ‘_ğ‘˜(exp(o^2)) - (âˆ‘_o o*exp(o)/âˆ‘_ğ‘˜(exp(o)))^2

âˆ‘_o o^2*exp(o^2)/âˆ‘_ğ‘˜(exp(o^2)) - âˆ‘_o o^2*exp(2*o)/âˆ‘_ğ‘˜(exp(2*o))

âˆ‘_o [ o^2*exp(o^2)/âˆ‘_ğ‘˜(exp(o^2)) - o^2*exp(2*o)/âˆ‘_ğ‘˜(exp(2*o)) ]

factor out o^2:

âˆ‘_o [ o^2 ( exp(o^2)/âˆ‘_ğ‘˜(exp(o^2)) - exp(2*o)/âˆ‘_ğ‘˜(exp(2*o)) ]
"""


# Take 2 -- attempt in sympy

from sympy import *
from sympy.abc import x,y,z

init_printing(use_unicode=True)

f, g, h = symbols('f g h', cls=Function)
softmax = symbols('softmax', cls=Function)
loss_def = symbols('loss_def', cls=Function)
loss = symbols('loss', cls=Function)
y_h = symbols('y_h', cls=Function, cls=seq)

j, k, q = symbols('x o j k q', integer=True)
# y, y_h = symbols('y y_h')

o = sympy.IndexedBase("o")
y = sympy.IndexedBase("y")

k = Idx("k", (1, q))
j = Idx("j", (1, q))

# EQN 3.4.3 Choice Model Softmax
y_h = exp(o[j])/Sum(exp(o[k]), k)
softmax = exp(o[j])/Sum(exp(o[k]),k)

# Cross Entropy Loss
loss_def = -1*summation(y*log(y_h), (j, 1, q))

loss = -1*summation(y*log(softmax), (j, 1, q))

diff(o[j]/Sum(o, k), o[j])

loss = -Sum(
        y[j]*
        log(
            exp(o[j])/
            Sum(exp(o[k]),k)
    ), j)

# 3.4.9 simplified loss
loss_1 = log( Sum(exp(o[k], k) ) - Sum(y[j]*o[j], j)

diff(loss_1, o[j])
"""
Out[273]:
  q
 ____
 â•²
  â•²
   â•²   o[k]
   â•±  â„¯    â‹…Î´
  â•±          j,k     q
 â•±                  ___
 â€¾â€¾â€¾â€¾               â•²
k = 0                â•²
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -   â•±   y[j]
    q               â•±
   ___              â€¾â€¾â€¾
   â•²               j = 0
    â•²    o[k]
    â•±   â„¯
   â•±
   â€¾â€¾â€¾
  k = 0


simplifies to:

     o[j]
    â„¯
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  -  y[j]
    q
   ___
   â•²
    â•²    o[k]
    â•±   â„¯
   â•±
   â€¾â€¾â€¾
  k = 0

because:
    y[k]=0, where k!=j

    partial derivative wrt o_j of Sum_k e^o_j = e^o_j
"""

doj_loss = exp(o[j])/Sum(exp(o[k]), k) - y[j]

diff(doj_loss, o[j])
"""
          q
         ____
         â•²
          â•²
   o[j]    â•²   o[k]
  â„¯    â‹…   â•±  â„¯    â‹…Î´
          â•±          j,k
         â•±
         â€¾â€¾â€¾â€¾                  o[j]
        k = 1                 â„¯
- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ + â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                   2         q
      â›  q        â         ___
      âœ ___       âŸ         â•²
      âœ â•²         âŸ          â•²    o[k]
      âœ  â•²    o[k]âŸ          â•±   â„¯
      âœ  â•±   â„¯    âŸ         â•±
      âœ â•±         âŸ         â€¾â€¾â€¾
      âœ â€¾â€¾â€¾       âŸ        k = 1
      âk = 1      â 

simplifies to (as described above):
-exp(o[j])**2/Sum(o[k], k)**2 + exp(o[j])/Sum(exp(o[k]), k):

      2â‹…o[j]          o[j]
     â„¯               â„¯
- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ + â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              2     q
  â›  q       â     ___
  âœ ___      âŸ     â•²
  âœ â•²        âŸ      â•²    o[k]
  âœ  â•²       âŸ      â•±   â„¯
  âœ  â•±   o[k]âŸ     â•±
  âœ â•±        âŸ     â€¾â€¾â€¾
  âœ â€¾â€¾â€¾      âŸ    k = 1
  âk = 1     â 


equivalent to:
exp(o[j])/Sum(o[k], k) * (1 - exp(o[j])/Sum(o[k], k))

or:
softmax*(1-softmax)

THUS:
d**2/d_o[j](loss) = softmax(o)[j]*(1-softmax(o)[j])
"""









# Q 1.2. Compute the variance of the distribution given by  softmax(ğ¨)  and show that it matches the second derivative computed above.

from sympy.stats import ContinuousRV, DiscreteRV, P, E
from sympy.stats import variance as V

X = DiscreteRV(x, exp(x)/Sum(exp(x), k))

# E(X)
"""
  âˆ
_____
â•²
 â•²
  â•²    â§x
   â•²   âªâ”€  for x = âŒŠxâŒ‹ âˆ§ x < âˆ
   â•±   â¨q
  â•±    âª
 â•±     â©0       otherwise
â•±
â€¾â€¾â€¾â€¾â€¾
x = -âˆ
"""

# V(X)
"""
    âˆ
__________
â•²
 â•²
  â•²        â§              2
   â•²       âªâ›      âˆ     â
    â•²      âªâœ     ____   âŸ
     â•²     âªâœ     â•²      âŸ
      â•²    âªâœ      â•²     âŸ
       â•²   âªâœ       â•²   xâŸ
        â•²  âªâœx -    â•±   â”€âŸ
        â•±  â¨âœ      â•±    qâŸ
       â•±   âªâœ     â•±      âŸ
      â•±    âªâœ     â€¾â€¾â€¾â€¾   âŸ
     â•±     âªâ    x = -âˆ  â 
    â•±      âªâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  for x = âŒŠxâŒ‹ âˆ§ x < âˆ
   â•±       âª       q
  â•±        âª
 â•±         â©       0              otherwise
â•±
â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾
  x = -âˆ
"""
# not giving us much to go on

# retry from where we left off in 1.1.
sm = symbols('sm', cls=Function)

sm = exp(x)/Integral(exp(x))

""" attempt #2
Start with second derivative and transform it into the variance

d**2/d_o[j](loss) = softmax(o)[j]*(1-softmax(o)[j])

Expectation of softmax(ğ¨)
E[softmax(ğ¨)]= Sum(softmax(o), k)/k
"""


"""
2. Assume that we have three classes which occur with equal probability, i.e., the probability vector is  (13,13,13) .

    1.What is the problem if we try to design a binary code for it?

    2.Can you design a better code? Hint: what happens if we try to encode two independent observations? What if we encode  ğ‘›  observations jointly?
-----------------------------------------------------------------------------------------------

2.1.
One of the fundamental theorems of information theory states that in order to encode
data drawn randomly from the distribution  ğ‘ƒ , we need at least  ğ»[ğ‘ƒ]  â€œnatsâ€ to encode
it. If you wonder what a â€œnatâ€ is, it is the equivalent of bit but when using a code
with base  ğ‘’  rather than one with base 2. Thus, one nat is  1/log(2)â‰ˆ1.44  bit.

P ~ Uniform([1/3, 1/3, 1/3])

So by the information theory
ğ»[ğ‘ƒ] = âˆ‘_ğ‘— âˆ’ğ‘ƒ(ğ‘—)logğ‘ƒ(ğ‘—)

We need 1.09 nats to encode the distribution P:

[ins] In [428]: -1/3*log(1/3)*3
Out[428]: 1.09861228866811

Which comes out to 1.58 bits:
[nav] In [431]: -1/3*log(1/3)*3*1.44
Out[431]: 1.58200169568208

Therefore binary (1 bit) is not enough capacity with which to encode the distribution.


2.2.
2-bits are the minimum bits to losslessly encode a single sample from P.
If we wish to encode n samples, then we require 2n bits.
"""

"""
3. Softmax is a misnomer for the mapping introduced above (but everyone in deep learning uses it). The real softmax is defined as  RealSoftMax(ğ‘,ğ‘)=log(exp(ğ‘)+exp(ğ‘)) .

    1. Prove that  RealSoftMax(ğ‘,ğ‘)>max(ğ‘,ğ‘) .

    2. Prove that this holds for  1/ğœ†*RealSoftMax(ğœ†ğ‘,ğœ†ğ‘) , provided that  ğœ†>0 .

    3. Show that for  ğœ†â†’ âˆ  we have  1/ğœ†*RealSoftMax(ğœ†ğ‘,ğœ†ğ‘)â†’ max(ğ‘,ğ‘) .

    4. What does the soft-min look like?

    5. Extend this to more than two numbers.
-----------------------------------------------------------------------------------------------

"""

# 3.1. Prove that  RealSoftMax(ğ‘,ğ‘)>max(ğ‘,ğ‘) .
a, b = symbols('a b')
StrictGreaterThan(log(exp(a) + exp(b)) , Max(a, b))
"""
       â› a    bâ
    logââ„¯  + â„¯ â  > Max(a, b)

    assume (a > b)

    Max(a,b) -> a:

       â› a    bâ
    logââ„¯  + â„¯ â  > a

    (exponentiate both sides)

     a    b    a
    â„¯  + â„¯  > â„¯

    True
"""

# 2. Prove that this holds for  1/ğœ†*RealSoftMax(ğœ†ğ‘,ğœ†ğ‘) , provided that  ğœ†>0 .

a, b, ğœ† = symbols('a b ğœ†')
StrictGreaterThan( 1/ğœ†*log(ğœ†*exp(a) + ğœ†*exp(b)) , Max(a, b))

"""
Show:
   â›   a      bâ
logâğœ†â‹…â„¯  + ğœ†â‹…â„¯ â 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ > Max(a, b)
       ğœ†

Assume:
    a > b
    ğœ† > 0

(Max(a,b) -> a, b/c a > b)

   â›   a      bâ
logâğœ†â‹…â„¯  + ğœ†â‹…â„¯ â 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ > a
       ğœ†

   â›   a      bâ
logâğœ†â‹…â„¯  + ğœ†â‹…â„¯ â  > ğœ†a

(exp both sides)

   a      b    ğœ†a
ğœ†â‹…â„¯  + ğœ†â‹…â„¯  > â„¯

 a    b        ğœ†a
â„¯  + â„¯  > 1/ğœ†â‹…â„¯

"""

3. Softmax is a misnomer for the mapping introduced above (but everyone in deep learning uses it). The real softmax is defined as  RealSoftMax(ğ‘,ğ‘)=log(exp(ğ‘)+exp(ğ‘)) .

    2. Prove that this holds for  1/ğœ†*RealSoftMax(ğœ†ğ‘,ğœ†ğ‘) , provided that  ğœ†>0 .

    3. Show that for  ğœ†â†’ âˆ  we have  1/ğœ†*RealSoftMax(ğœ†ğ‘,ğœ†ğ‘)â†’ max(ğ‘,ğ‘) .
